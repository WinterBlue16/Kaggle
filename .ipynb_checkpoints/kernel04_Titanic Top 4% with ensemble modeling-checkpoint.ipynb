{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Top 4% with ensemble modeling \n",
    "<br>\n",
    "\n",
    "## Yassine Ghouzam, PhD\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "13/07/2017\n",
    "- **1 Introduction**\n",
    "- **2 Load and check data**\n",
    "    - 2.1 load data \n",
    "    - 2.2 Outlier detection \n",
    "    - 2.3 joining train and test set\n",
    "    - 2.4 check for null and missing values \n",
    "- **3 Feature analysis**\n",
    "    - 3.1 Numerical values \n",
    "    - 3.2 Categorical values\n",
    "- **4 Filling missing Values**\n",
    "    - 4.1 Age\n",
    "- **5 Feature engineering** \n",
    "    - 5.1 Name/Title\n",
    "    - 5.2 Family Size\n",
    "    - 5.3 Cabin \n",
    "    - 5.4 Ticket\n",
    "- **6 Modeling** \n",
    "    - 6.1 Simple modeling \n",
    "        - 6.1.1 Cross validate models\n",
    "        - 6.1.2 Hyperparameter tunning for best models\n",
    "        - 6.1.3 Plot learning curves\n",
    "        - 6.1.4 Feature importance of the tree based classifiers \n",
    "     - 6.2 Ensemble modeling \n",
    "         - 6.2.1 Combining models\n",
    "     - 6.3 Prediction \n",
    "         - 6.3.1 Predict and Submit results\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## 1. Introduction \n",
    "<br>\n",
    "This is my first kernel at Kaggle. I choosed the Titanic competition which is a good way to introduce feature engineering and ensemble modeling. Firstly, I will display some feature analyses then ill focus on the feature engineering. Last part concerns modeling and predicting the survival on the Titanic using and voting procedure. \n",
    "<br>\n",
    "\n",
    "This script follows three main parts:\n",
    "<br>\n",
    "\n",
    "- Feature analysis\n",
    "- Feature engineering \n",
    "- Modeling \n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and check data\n",
    "<br>\n",
    "\n",
    "### 2.1 Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "##### Load train and Test\n",
    "\n",
    "train = pd.read_csv('/data/train.csv')\n",
    "test = pd.read_csv('/data/test.csv')\n",
    "IDtest = test['PassengerId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection \n",
    "\n",
    "def detect_outliers(df, n, features):\n",
    "    '''\n",
    "    Takes a dataframe df of features and returns a list the indices\n",
    "    corresponding to the observations containing more than n outliers according to the Tukey method.\n",
    "    '''\n",
    "    outlier_indices=[]\n",
    "    \n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[col], 75)\n",
    "        # Interquartile range(IQR)\n",
    "        IQR = Q3-Q1\n",
    "        \n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        \n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index\n",
    "        \n",
    "        # append the found outlier indices for col the list of outlier indices\n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "        \n",
    "    # append the found outlier indices for col to the list of outlier indices\n",
    "    outlier_indices = Counter(outlier_indices)\n",
    "    multiple_outliers = list(k for k, v in outlier_indices.items() if v > n)\n",
    "        \n",
    "    return multiple_outliers\n",
    "    \n",
    "# detect outliers from Age, SibSp, Parch and Fare\n",
    "Outliers_to_drop = detect_outliers(train, 2, ['Age', 'SibSp', 'Parch', 'Fare'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since outliers can have a dramatic effect on the prediction(espacially for regression problems), I choosed to manage them. \n",
    "<br>\n",
    "\n",
    "I used the Tukey method(Tukey JW, 1977) to detect outliers which defines an interquartile range comprised between the 1st and 3rd quartile of the distribution values(IQR). An outlier is a row that have a feature value outside the (IQR +- an outlier step).\n",
    "<br>\n",
    "\n",
    "I decided to detect outliers from the numerical values feature(Age, SibSp, Sarch and Fare).Then, i considered outliers as rows that have at least two outlied numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[Outliers_to_drop] # show the outliers row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We detect 10 outliers. The 28, 89 and 342 passenger have an high Ticket Fare The 7 others have very high values of SibSP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop outliers\n",
    "train = train.drop(Outliers_to_drop).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 joining train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join train and test datasets in order to obtain the same number of features during categorical conversion\n",
    "train_len = len(train)\n",
    "dataset = pd.concat(objs=[train, test], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 check for null and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill empty and NaNs values with NaN\n",
    "dataset = dataset.fillna(np.nan)\n",
    "\n",
    "# Check for null values\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age and Cabin features have an important part of missing values.\n",
    "<br>\n",
    "\n",
    "**Survived missing values correspond to the join testing dataset(Survived column doesn't exist in test set and has been replace by NaN values when concatenating the train and test set)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infos\n",
    "train.info()\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summerize data\n",
    "# Summerize and statistics\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature analysis\n",
    "<br>\n",
    "\n",
    "### 3.1 Numerical values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix between numerical values (SibSp Parch Age and Fare values) and Survived\n",
    "g = sns.heatmap(train[[\"Survived\", \"SibSp\", \"Parch\", \"Age\", \"Fare\"]].corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only Fare feature seems to have a significative correlation with the survival probability.\n",
    "<br>\n",
    "\n",
    "It doesn't mean that the other features are not useful. Subpopulations in these features can be correlated with the survival. To determine this, we need to explore in detail these features\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**SibSP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore SibSp feature vs Survived\n",
    "g = sns.factorplot(x=\"SibSp\", y=\"Survived\", data=train, kind=\"bar\", size=6, palette='muted')\n",
    "g.despine(left=True)\n",
    "g=g.set_ylabels(\"Survived probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that passengers having a lot of siblings/spouses have less chance to survive\n",
    "<br>\n",
    "\n",
    "Single passengers(0 SibSP) or with two other persons(SibSP 1 or 2) have more chance to survive\n",
    "<br>\n",
    "\n",
    "This observation is quite interestin, we can consider a new feature describing these categories(See feature engineering)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Parch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Parch feature vs Survived\n",
    "g = sns.factorplot(x='Parch', y='Survived', data=train, kind='bar', size=6, palette='muted')\n",
    "g.despine(left=True)\n",
    "g = g.set_ylabels('survived probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small families have more chance to survive, more than single(Parch 0), medium(Parch 3, 4) and large families(Parch 5, 6).\n",
    "<br>\n",
    "\n",
    "Be careful there is an important standard deviation in the survival of passengers with 3 parents/children\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Age**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Age vs Survived\n",
    "g = sns.FaceGrid(train, col='Survived')\n",
    "g = g.map(sns.distplot, 'Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age distribution seems to be a tailed distribution, maybe a gaussian distribution.\n",
    "<br>\n",
    "\n",
    "We notice that age distributions are not the same in the survived and not survived subpopulations. Indeed, there is a peak corresponding to young passengers, that have survived. We also see that passengers between 60-80 have less survived.\n",
    "<br>\n",
    "\n",
    "So, even if \"Age\" is not correlated with \"Survived\", we can see that there is age categories of passengers that of have more or less chance to survive.\n",
    "<br>\n",
    "\n",
    "It seems that very young passengers have more chance to survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Age distribution \n",
    "g = sns.kdeplot(train[\"Age\"][(train[\"Survived\"]==0) & (train[\"Age\"].notnull())], color=\"Red\", shade=True)\n",
    "g = sns.kdeplot(train['Age'][(train['Survived']==1) & (train[\"Age\"].notnull())], ax=g, color=\"Blue\", shade=True)\n",
    "g.set_xlabel(\"Age\")\n",
    "g.set_ylabel(\"Frequency\")\n",
    "g = g.legend([\"Not Survived\", \"Survived\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we superimpose the two densities, we cleary see a peak correponsing(between 0 and 5) to babies and very young childrens.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Fare**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Fare'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Fare missing values with the median value\n",
    "dataset['Fare'] = dataset['Fare'].fillna(dataset['Fare'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have one missing value, I decided to fill with the median value which will not have an important effect on the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Fare distribution\n",
    "g = sns.distplot(dataset['Fare'], color='m', label='Skewness : %.2f'%(dataset['Fare'].skew()))\n",
    "g = g.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, Fare distribution is very skewed. This can lead to overweigth very high values in the model, even if it is scaled.\n",
    "<br>\n",
    "\n",
    "In this case, it is better to transform it with the log function to reduce this skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log to Fare to reduce skewness distribution\n",
    "dataset['Fare'] = dataset['Fare'].map(lambda i : np.log(i) if i > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot(dataset['Fare'], color='b', label='Skewness : %.2f'%(dataset['Fare'].skew()))\n",
    "g = g.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skewness is clearly reduced after the log transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Categorical values\n",
    "#### Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.barplot(x='Sex', y='Survived', data=train)\n",
    "g = g.set_ylabel('Survival Probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['Sex', 'Survived']].groupby('Sex').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clearly obvious that Male have less chance to survive than Female.\n",
    "<br>\n",
    "\n",
    "So Sex, might play an important role in the prediction of the survival.\n",
    "<br>\n",
    "\n",
    "For those who have seen the Titanic movie(1997), I am sure, we all remember this sentence during the evacuation:\"Women and children first\".\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Pclass vs Survived\n",
    "g = sns.factorplot(x='Pclass', y='Survived', data=train, kind='bar', size=6, palette='muted')\n",
    "g.despine(left=True)\n",
    "g = g.set_ylabels('survival probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Pclass vs Survived by Sex\n",
    "g = sns.factorplot(x='Pclass', y='Survived', hue='Sex', data=train, \n",
    "                  size=6, kind='bar', palette='muted')\n",
    "g.despine(left=True)\n",
    "g = g.set_ylabels('survival probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The passenger survival is not the same in the 3 classes. First class passengers have more chance to survive than second class and third class passengers.\n",
    "<br>\n",
    "\n",
    "This trend is conserved when we look at both male and female passengers.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Embarked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Embarked'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Embarked nan values of dataset set with 'S' most frequent value\n",
    "dataset['Embarked'] = dataset['Embarked'].fillna('S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have two missing values, I decided to fill them with the most frequent value of 'Embarked'(S)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Embarked vs Survived\n",
    "g = sns.factorplot(x='Embarked', y='Survived', data=train, \n",
    "                  size=6, kind='bar', palette='muted')\n",
    "g.despine(left=True)\n",
    "g = g.set_ylabels('survival probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that passenger coming from Cherbourg(C) have more chance to survive.\n",
    "<br>\n",
    "\n",
    "My hypothesis is that the proportion of first class passengers is higher for those who came from Cherbourg than Queenstown(Q), Southampton(S).\n",
    "<br>\n",
    "\n",
    "Let's see the Pclass distribution vs Embarked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Pclass vs Embarked\n",
    "g = sns.factorplot('Pclass', col='Embarked', data=train, \n",
    "                  size=6, kind='count', palette='muted')\n",
    "g.despine(left=True)\n",
    "g = g.set_ylabels('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the third class is the most frequent for passenger coming from Southampton(S) and Queenstown(Q), whereas Cherbourg passengers are mostly in first class which have the highest survival rate.\n",
    "<br>\n",
    "\n",
    "At this point, I can't explain why first class have an higher survival rate. My hypothesis is that first class passengers were prioritised during the evacuation due to their influence.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## 4. Filling missing Values\n",
    "### 4.1 Age\n",
    "As we see, Age column contains 256 missing values in the whole dataset.\n",
    "<br>\n",
    "\n",
    "Since there is subpopulations that have more chance to survive(children for example), it is preferable to keep the age feature and to impute the missing values.\n",
    "<br>\n",
    "\n",
    "To adress this problem, I looked at the most correlated features with Age(Sex, Parch, Pclass and SibSP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Age vs Sex, Parch, Pclass and SibSP\n",
    "g = sns.factorplot(y='Age', x='Sex', data=dataset, kind='box')\n",
    "g = sns.factorplot(y='Age', x='Sex', hue='Pclass', data=dataset, kind='box')\n",
    "g = sns.factorplot(y='Age', x='Parch', data=dataset, kind='box')\n",
    "g = sns.factorplot(y='Age', x='SibSp', data=dataset, kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age distribution seems to be the same in Male and Female subpopulations, so Sex is not informative to predict Age. \n",
    "<br>\n",
    "\n",
    "However, 1st class passengers are older than 2nd class passengers who are also older than 3rd class passengers. \n",
    "<br>\n",
    "\n",
    "Moreover, the more a passengers has parents/children the older he is and the more a passenger has siblings/spouses the younger he is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Sex into categorical value 0 for male and 1 for female\n",
    "dataset['Sex'] = dataset['Sex'].map({'male':0, 'female':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.heatmap(dataset[['Age', 'Sex', 'SibSp', 'Parch', 'Pclass']].corr(), cmap='BrBG', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation map confirms the factorplots observations except for Parch. Age is not correlated with Sex, but is negatively correlated with Pclass, Parch and SibSp.\n",
    "<br>\n",
    "\n",
    "In the plot of Age in function of Parch, Age is growing with the number of parents/children. But the general correlation is negative.\n",
    "<br>\n",
    "\n",
    "So, I decided to use SibSp, Parch and Pclass in order to impute the missing ages.\n",
    "<br>\n",
    "\n",
    "The strategy is to fill Age with the median age of similar rows according to Pclass, Parch and SibSp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing value of Age\n",
    "\n",
    "## Fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n",
    "# Index of NaN age rows\n",
    "index_NaN_age = list(dataset['Age'][dataset['Age'].isnull()].index)\n",
    "\n",
    "for i in index_NaN_age:\n",
    "    age_med = dataset['Age'].median()\n",
    "    age_pred = dataset['Age'][((dataset['SibSp']==dataset.iloc[i]['SibSp']) & (dataset['Parch'] == dataset.iloc[i]['Parch']) \n",
    "                               & (dataset['Pclass'] == dataset.iloc[i]['Pclass']))].median()\n",
    "    if not np.isnan(age_pred):\n",
    "        dataset['Age'].iloc[i] = age_pred\n",
    "    else:\n",
    "        dataset['Age'].iloc[i] = age_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.factorplot(x='Survived', y='Age', data=train, kind='box')\n",
    "g = sns.factorplot(x='Survived', y='Age', data=train, kind='violin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No different between median value of age in survived and not survived subpopulation.\n",
    "<br>\n",
    "\n",
    "But in the violin plot of survived passengers, we still notice that very young passengers have higher survival rate.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## 5. Feature engineering\n",
    "### 5.1 Name/Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Name'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Name feature contains information on passenger's title.\n",
    "<br>\n",
    "\n",
    "Since some passenger with distingused title may be preferred during the evacuation, it is interesting to add them to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Title from Name\n",
    "dataset_title = [i.split(',')[1].split('.')[0].strip() for i in dataset['Name']]\n",
    "dataset['Title'] = pd.Series(dataset_title)\n",
    "dataset['Title'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.countplot(x='Title', data=dataset)\n",
    "g = plt.setp(g.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 17 titles in the dataset, most of them are very rare and we can group them in 4 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to categorical values Title\n",
    "dataset['Title'] = dataset['Title'].replace(['Lady', 'the Countess', 'Countess', 'Capt', 'Col', 'Don', 'Dr',\n",
    "                                             'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "dataset['Title'] = dataset['Title'].map({'Master':0, 'Miss':1, 'Ms':1, 'Mme':1, 'Mlle':1, 'Mrs':1, 'Mr':2, 'Rare':3})\n",
    "dataset['Title'] = dataset['Title'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.countplot(dataset['Title'])\n",
    "g = g.set_xticklabels(['Master', 'Miss/Ms/Mme/Mlle/Mrs', 'Mr', 'Rare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.factorplot(x='Title', y = 'Survived', data=dataset, kind='bar')\n",
    "g = g.set_xticklabels(['Master', 'Miss-Mrs', 'Mr', 'Rare'])\n",
    "g = g.set_ylabels('survival probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Women and children first\"\n",
    "<br>\n",
    "\n",
    "It is interesting to note that passengers with rare title have more chance to survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Name variable\n",
    "dataset.drop(labels=['Name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Family size\n",
    "We can imagine that large families will have more difficulties to evacuate, looking for theirs sisters/brothers/parents during the evacuation. So, I choosed to create a 'Fize'(Family size) feature which is the sum of SibSp, Parch and 1(including the passenger)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a family size descriptor from SibSp and Parch \n",
    "dataset['Fsize']  = dataset['SibSp'] + dataset['Parch'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.factorplot(x='Fsize', y='Survived', data=dataset)\n",
    "g = g.set_ylabels('Survival Probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The family size seems to play an important role, survival probabilty is worst for large families.\n",
    "<br>\n",
    "\n",
    "Additionally,I decided to created 4 categories of family size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature of family size\n",
    "dataset['Single'] = dataset['Fsize'].map(lambda s: 1 if s == 1 else 0)\n",
    "dataset['SmallF'] = dataset['Fsize'].map(lambda s: 1 if s == 2 else 0)\n",
    "dataset['MedF'] = dataset['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n",
    "dataset['LargeF'] = dataset['Fsize'].map(lambda s : 1 if s >= 5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.factorplot(x='Single', y='Survived', data=dataset, kind='bar')\n",
    "g = g.set_ylabels('Survival Probability')\n",
    "g = sns.factorplot(x='SmallF', y='Survived', data=dataset, kind='bar')\n",
    "g = g.set_ylabels('Survival Probability')\n",
    "g = sns.factorplot(x='MedF', y='Survived', data=dataset, kind='bar')\n",
    "g = g.set_ylabels('Survival Probability')\n",
    "g = sns.factorplot(x='LargeF', y='Survived', data=dataset, kind='bar')\n",
    "g = g.set_ylabels('Survival Probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factorplots of family size categories show that Small and Medium families have more chance to survive than single passenger and large families. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to indicator values Title and Embarked\n",
    "dataset = pd.get_dummies(dataset, columns = ['Title'])\n",
    "dataset = pd.get_dummies(dataset, columns = ['Embarked'], prefix='Em')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we have 22 features.\n",
    "<br>\n",
    "\n",
    "### 5.3 Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Cabin'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Cabin'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Cabin'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cabin feature column contains 292 values and 1007 missing values\n",
    "<br>\n",
    "\n",
    "I supposed that passengers without a cabin have a missing value displayed instead of the cabin number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Cabin'][dataset['Cabin'].notnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the Cabin number by the type of cabin 'X' if not\n",
    "dataset['Cabin'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first letter of the cabin indicates the Desk, I choosed to keep this information only, since it indicates the probable location of the passenger in the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.countplot(dataset['Cabin'], order=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'T', 'X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.factorplot(y='Survived', x='Cabin', data=dataset, kind='bar', order=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'T', 'X'])\n",
    "g = g.set_ylabels('Survival Probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the low number of passenger that have a cabin, survival probabilities have an important standard deviation and we can't distinguish between survival probability of passengers in the different desks.\n",
    "<br>\n",
    "\n",
    "But we can see that passengers with a cabin have generally more chance to survive than passengers without (X).\n",
    "<br>\n",
    "\n",
    "It is particularly true for cabin B, C, D, E and F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.get_dummies(dataset, columns=['Cabin'], prefix='Cabin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Ticket'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It could mean that tickets sharing the same prefixes could be booked for cabins placed together. It could therefore lead to the actual placement of the cabins within the ship.\n",
    "<br>\n",
    "\n",
    "Tickets with same prefixes may have a similar class and survival.\n",
    "<br>\n",
    "\n",
    "So I decided to replace the Ticket feature column by the ticket prefixe. Which may be more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Treat Ticket by extracting the ticket prefix. When there is no prefix it returns X.\n",
    "\n",
    "Ticket=[]\n",
    "for i in list(dataset.Ticket):\n",
    "    if not i.isdigit():\n",
    "        Ticket.append(i.replace('.', '').replace('/', '').strip().split(' ')[0]) # Take prefix\n",
    "        \n",
    "    else:\n",
    "        Ticket.append('X')\n",
    "    \n",
    "dataset['Ticket'] = Ticket\n",
    "dataset['Ticket'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.get_dummies(dataset, columns=['Ticket'], prefix='T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categorical values for Pclass\n",
    "dataset['Pclass'] = dataset['Pclass'].astype('category')\n",
    "dataset = pd.get_dummies(dataset, columns=['Pclass'], prefix='Pc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless variables\n",
    "dataset.drop(labels = ['PassengerId'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate train dataset and test dataset\n",
    "\n",
    "train = dataset[:train_len]\n",
    "test = dataset[train_len:]\n",
    "test.drop(labels=['Survived'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seperate train features and label\n",
    "\n",
    "train['Survived'] = train['Survived'].astype(int)\n",
    "\n",
    "Y_train = train['Survived']\n",
    "X_train = train.drop(labels=['Survived'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Simple modeling\n",
    "### 6.1.1 Cross validate models\n",
    "I compared 10 popular classifiers and evaluate the mean accuracy of each of them by a stratified kfold cross validation procedure.\n",
    "<br>\n",
    "- SVC\n",
    "- Decision Tree\n",
    "- AdaBoost\n",
    "- Random Forest\n",
    "- Extra Trees\n",
    "- Gradient Boosting\n",
    "- Multiple layer perceptron(neural network)\n",
    "- KNN\n",
    "- Logistic regression\n",
    "- Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validate model with Kfold stratified cross val\n",
    "kfold = StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling step Test differents algorithms\n",
    "random_state=2\n",
    "classifiers =[]\n",
    "classifiers.append(SVC(random_state=random_state))\n",
    "classifiers.append(DecisionTreeClassifier(random_state=random_state))\n",
    "classifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state), random_state=random_state, \n",
    "                                      learning_rate=0.1))\n",
    "classifiers.append(RandomForestClassifier(random_state=random_state))\n",
    "classifiers.append(ExtraTreesClassifier(random_state=random_state))\n",
    "classifiers.append(GradientBoostingClassifier(random_state=random_state))\n",
    "classifiers.append(MLPClassifier(random_state=random_state))\n",
    "classifiers.append(KNeighborsClassifier())\n",
    "classifiers.append(LogisticRegression(random_state=random_state))\n",
    "classifiers.append(LinearDiscriminantAnalysis())\n",
    "\n",
    "cv_results=[]\n",
    "for classifier in classifiers:\n",
    "    cv_results.append(cross_val_score(classifier, X_train, y = Y_train, scoring='accuracy', cv=kfold, n_jobs=4))\n",
    "    \n",
    "cv_means=[]\n",
    "cv_std=[]\n",
    "\n",
    "for cv_result in cv_results:\n",
    "    cv_means.append(cv_result.mean())\n",
    "    cv_std.append(cv_result.std())\n",
    "\n",
    "cv_res = pd.DataFrame({'CrossValMeans': cv_means, 'CrossValerrors': cv_std, 'Algorithm': ['SVC', 'DecisionTree', 'AdaBoost',\n",
    "                                                                                          'RandomForest', 'ExtraTrees', \n",
    "                                                                                         'GradientBoosting', 'Multiple layer perceptron',\n",
    "                                                                                         'KNeighbors', 'LogisticRegression', \n",
    "                                                                                         'LinearDiscriminantAnalysis']})\n",
    "\n",
    "g = sns.barplot('CrossValMeans', 'Algorithm', data=cv_res, palette='Set3', orient='h', **{'xerr':cv_std})\n",
    "g.set_xlabel('Mean Accuracy')\n",
    "g = g.set_title('Cross validation scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to choose the SVC, Adaboost, RandomForest, ExtraTrees and the GradientBoosting classfiers for the ensemble modeling.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### 6.1.2 Hyperparameter tunning for best models\n",
    "I performed a grid search optimization for AdaBoost, ExtraTrees, RandomForest, GradientBoosting and SVC classifiers.\n",
    "<br>\n",
    "\n",
    "I set the 'n_jobs'parameter to 4 since I have 4 cpu. The computation time is clearly reduced.\n",
    "<br>\n",
    "\n",
    "But be careful, this step can take a long time, It took me 15min in total on 4 cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### META MODELING WITH ADABOOST, RF, EXTRATREES and GRADIENTBOOSTING\n",
    "\n",
    "# AdaBoost\n",
    "DTC = DecisionTreeClassifier()\n",
    "adaDTC = AdaBoostClassifier(DTC, random_state=7)\n",
    "\n",
    "ada_param_grid = {'base_estimator__criterion': ['gini', 'entropy'],\n",
    "                 'base_estimator__splitter' : ['best', 'random'],\n",
    "                 'algorithm': ['SAMME', 'SAMME.R'],\n",
    "                 'n_estimators': [1,2], \n",
    "                 'learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 1.5]}\n",
    "\n",
    "gsadaDTC = GridSearchCV(adaDTC, param_grid=ada_param_grid, cv=kfold, scoring='accuracy', n_jobs=4, verbose=1)\n",
    "\n",
    "gsadaDTC.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsadaDTC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExtraTrees\n",
    "ExtC = ExtraTreesClassifier()\n",
    "\n",
    "\n",
    "## Search grid for optimal parameters\n",
    "ex_param_grid = {'max_depth': [None],\n",
    "                'max_features':[1,3,10],\n",
    "                'min_samples_split':[2,3,10],\n",
    "                'min_samples_leaf':[1,3,10],\n",
    "                'bootstrap':[False],\n",
    "                'n_estimators':[100,300],\n",
    "                'criterion':['gini']}\n",
    "\n",
    "gsExtC = GridSearchCV(ExtC, param_grid=ex_param_grid, cv=kfold, scoring='accuracy', n_jobs=4, verbose=1)\n",
    "\n",
    "gsExtC.fit(X_train, Y_train)\n",
    "\n",
    "ExtC_best = gsExtC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsExtC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFC Parameters tunning\n",
    "RFC = Random"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
